{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:09:58.405404Z",
     "start_time": "2021-01-03T05:09:57.222677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# 策略梯度算法\n",
    "# 2020.5.22\n",
    "#\n",
    "# cartpole 的state是一个4维向量，分别是位置，速度，杆子的角度，加速度；action是二维、离散，即向左/右推杆子\n",
    "# 每一步的reward都是1  游戏的threshold是475\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Pytorch REINFORCE example')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',help='discount factor(default:0.99)')\n",
    "parser.add_argument('--seed',type=int, default=543, metavar='N',help='random seed (default: 543)')\n",
    "parser.add_argument('--render',action='store_false',help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)    # 策略梯度算法方差很大，设置seed以保证复现性\n",
    "print('observation space:',env.observation_space)\n",
    "print('action space:',env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:09:58.498709Z",
     "start_time": "2021-01-03T05:09:58.410390Z"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    ##  离散空间采用了 softmax policy 来参数化策略\n",
    "    def __init__(self):\n",
    "        super(Policy,self).__init__()\n",
    "        self.affline1 = nn.Linear(4,128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affline2 = nn.Linear(128,2)  # 两种动作\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affline1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affline2(x)\n",
    "        return F.softmax(action_scores,dim=1)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(),lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()  # 非负的最小值，使得归一化时分母不为0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    ## 选择动作，这个动作不是根据Q值来选择，而是使用softmax生成的概率来选\n",
    "    #  不需要epsilon-greedy，因为概率本身就具有随机性\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    #print(state.shape)   torch.size([1,4])\n",
    "    probs = policy(state)\n",
    "    # print(probs)\n",
    "    # print(probs.log())\n",
    "    m = Categorical(probs)      # 生成分布\n",
    "    action = m.sample()           # 从分布中采样\n",
    "    #print(m.log_prob(action))   # m.log_prob(action)相当于probs.log()[0][action.item()].unsqueeze(0)\n",
    "    policy.saved_log_probs.append(m.log_prob(action))    # 取对数似然 logπ(s,a)\n",
    "    return action.item()         # 返回一个元素值\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0,R)        # 将R插入到指定的位置0处\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)     # 归一化\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)          # 损失函数为交叉熵\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()          # 求和\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]          # 清空episode 数据\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in range(1000):        # 采集（训练）最多1000个序列\n",
    "        state, ep_reward = env.reset(),0    # ep_reward表示每个episode中的reward\n",
    "        #print(state.shape)\n",
    "        for t in range(1, 1000):\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:   # 大于游戏的最大阈值475时，退出游戏\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:30:14.933091Z",
     "start_time": "2021-01-03T05:09:58.504696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 78.00\tAverage reward: 13.40\n",
      "Episode 10\tLast reward: 13.00\tAverage reward: 15.85\n",
      "Episode 20\tLast reward: 29.00\tAverage reward: 15.56\n",
      "Episode 30\tLast reward: 74.00\tAverage reward: 23.43\n",
      "Episode 40\tLast reward: 23.00\tAverage reward: 27.15\n",
      "Episode 50\tLast reward: 81.00\tAverage reward: 33.31\n",
      "Episode 60\tLast reward: 158.00\tAverage reward: 48.46\n",
      "Episode 70\tLast reward: 67.00\tAverage reward: 70.22\n",
      "Episode 80\tLast reward: 108.00\tAverage reward: 78.24\n",
      "Episode 90\tLast reward: 205.00\tAverage reward: 102.00\n",
      "Episode 100\tLast reward: 81.00\tAverage reward: 97.53\n",
      "Episode 110\tLast reward: 283.00\tAverage reward: 127.79\n",
      "Episode 120\tLast reward: 138.00\tAverage reward: 179.01\n",
      "Episode 130\tLast reward: 258.00\tAverage reward: 187.36\n",
      "Episode 140\tLast reward: 191.00\tAverage reward: 214.65\n",
      "Episode 150\tLast reward: 241.00\tAverage reward: 244.87\n",
      "Episode 160\tLast reward: 203.00\tAverage reward: 282.34\n",
      "Episode 170\tLast reward: 337.00\tAverage reward: 319.27\n",
      "Episode 180\tLast reward: 125.00\tAverage reward: 246.30\n",
      "Episode 190\tLast reward: 231.00\tAverage reward: 245.15\n",
      "Episode 200\tLast reward: 253.00\tAverage reward: 258.94\n",
      "Episode 210\tLast reward: 500.00\tAverage reward: 297.23\n",
      "Episode 220\tLast reward: 500.00\tAverage reward: 378.59\n",
      "Episode 230\tLast reward: 500.00\tAverage reward: 427.31\n",
      "Episode 240\tLast reward: 500.00\tAverage reward: 448.85\n",
      "Episode 250\tLast reward: 488.00\tAverage reward: 455.19\n",
      "Episode 260\tLast reward: 278.00\tAverage reward: 428.96\n",
      "Episode 270\tLast reward: 500.00\tAverage reward: 446.34\n",
      "Episode 280\tLast reward: 500.00\tAverage reward: 467.87\n",
      "Solved! Running reward is now 475.1400855885911 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
